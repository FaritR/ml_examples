{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justm/anaconda3/envs/uw_env/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import fileinput\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justm/anaconda3/envs/uw_env/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Skipping line 24551: NULL byte detected. This byte cannot be processed in Python's native csv library at the moment, so please pass in engine='c' instead\n",
      "Skipping line 34985: field larger than field limit (131072)\n",
      "Skipping line 76012: field larger than field limit (131072)\n"
     ]
    }
   ],
   "source": [
    "for line in fileinput.input(['train.tsv'], inplace=True, backup='.bak'):\n",
    "    sys.stdout.write(re.sub(r'(?<!,)\"(?!,)',\"\", line))\n",
    "\n",
    "data = pd.read_csv('train.tsv', sep='\\t', engine='python',  error_bad_lines=False,\n",
    "                   header=None, index_col=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size=0.1, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d368560cb7e04a4a9b58266ed5f475db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "28031     None\n",
       "109442    None\n",
       "118025    None\n",
       "111164    None\n",
       "17914     None\n",
       "          ... \n",
       "85146     None\n",
       "111105    None\n",
       "35881     None\n",
       "12003     None\n",
       "35826     None\n",
       "Length: 113335, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "a_topic = defaultdict(Counter)\n",
    "def idf_row(row):\n",
    "    def prepare_tfidf(text, topic):\n",
    "        text = re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
    "        a_topic[topic] += Counter(text)\n",
    "    topics = row[3].split(',')\n",
    "    topics.append('100')\n",
    "    for topic in topics:\n",
    "        prepare_tfidf(row[1], topic)\n",
    "    \n",
    "data_train.progress_apply(idf_row, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102850"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_topic['100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('counter_title.pickle', 'wb') as f:\n",
    "    pickle.dump(a_topic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e51ff161de440d8a0b4037158fa8db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102850), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words_90 = []\n",
    "stop_words_60 = []\n",
    "stop_words_30 = []\n",
    "rare_words = []\n",
    "bad_rare_words = []\n",
    "for word in tqdm_notebook(a_topic['100'].most_common(102850)):\n",
    "    tmp = 0\n",
    "    tmp2 = 0\n",
    "    for i in range(100):\n",
    "        tmp1 = a_topic[str(i)].keys()\n",
    "        if word[0] in tmp1:\n",
    "            tmp += 1\n",
    "            tmp2 = max(tmp2, a_topic[str(i)][word[0]])\n",
    "    if tmp > 90:\n",
    "        stop_words_90.append(word)\n",
    "    if tmp > 60:\n",
    "        stop_words_60.append(word)\n",
    "    if tmp > 30:\n",
    "        stop_words_30.append(word)\n",
    "    if word[1] / tmp >= 3:\n",
    "        if tmp <= 10:\n",
    "            rare_words.append(word)\n",
    "    if tmp2 < 2:\n",
    "        bad_rare_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_answers = defaultdict(list)\n",
    "for word in rare_words:\n",
    "    for i in range(100):\n",
    "        if (word[0] in a_topic[str(i)].keys()) and (a_topic[str(i)][word[0]]>=3):\n",
    "            rare_answers[word].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429ff25ba9f64a68bbf65271a9dc4696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sum_theme = [sum(a_topic[str(i)].values()) for i in range(100)]\n",
    "words = list(a_topic['100'].keys())\n",
    "tf_arr = np.zeros((100, len(words)))\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in a_topic[str(i)]:\n",
    "            tf_arr[i, j] = a_topic[str(i)][words[j]] / sum_theme[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_arr = np.zeros(len(words))\n",
    "for j in range(len(words)):\n",
    "    idf_arr[j] = np.log(100 / sum([words[j] in a_topic[str(i)] for i in range(100)]))\n",
    "tfidf = np.array(tf_arr, copy = True)\n",
    "for k in range(len(words)):\n",
    "    tfidf[:,k] = tf_arr[:,k]  * idf_arr[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict = dict(zip(words, list(range(len(words)))))\n",
    "words_dict['украины']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 102850)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2e74c3e732450783af1d6120f8116b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "28031     None\n",
       "109442    None\n",
       "118025    None\n",
       "111164    None\n",
       "17914     None\n",
       "          ... \n",
       "85146     None\n",
       "111105    None\n",
       "35881     None\n",
       "12003     None\n",
       "35826     None\n",
       "Length: 113335, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_topic_text = defaultdict(Counter)\n",
    "def idf_row(row):\n",
    "    def prepare_tfidf(text, topic):\n",
    "        text = re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
    "        a_topic_text[topic] += Counter(text)\n",
    "    topics = row[3].split(',')\n",
    "    topics.append('100')\n",
    "    for topic in topics:\n",
    "        prepare_tfidf(row[2], topic)\n",
    "    \n",
    "data_train.progress_apply(idf_row, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836577"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_topic_text['100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('counter_text.pickle', 'wb') as f:\n",
    "    pickle.dump(a_topic_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bce57a8f6c4d10a6dd96cb642009c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=836577), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words1_90 = []\n",
    "stop_words1_70 = []\n",
    "stop_words1_50 = []\n",
    "rare_words1 = []\n",
    "bad_rare_words1 = []\n",
    "for word in tqdm_notebook(a_topic_text['100'].most_common(882102)):\n",
    "    tmp = 0\n",
    "    tmp2 = 0\n",
    "    for i in range(100):\n",
    "        tmp1 = a_topic_text[str(i)].keys()\n",
    "        if word[0] in tmp1:\n",
    "            tmp += 1\n",
    "            tmp2 = max(tmp2, a_topic_text[str(i)][word[0]])\n",
    "    if tmp > 90:\n",
    "        stop_words1_90.append(word)\n",
    "    if tmp > 70:\n",
    "        stop_words1_70.append(word)\n",
    "    if tmp > 50:\n",
    "        stop_words1_50.append(word)\n",
    "    if word[1] // tmp >= 10:\n",
    "        if tmp <= 10:\n",
    "            rare_words1.append(word)\n",
    "    if tmp2 < 5:\n",
    "        bad_rare_words1.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_answers1 = defaultdict(list)\n",
    "for word in rare_words1:\n",
    "    for i in range(100):\n",
    "        if (word[0] in a_topic_text[str(i)].keys()) and (a_topic_text[str(i)][word[0]]>=3):\n",
    "            rare_answers1[word].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6a977d845f40cc834cee1c30df89a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 836577)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_theme = [sum(a_topic_text[str(i)].values()) for i in range(100)]\n",
    "words = list(a_topic_text['100'].keys())\n",
    "tf_arr = np.zeros((100, len(words)))\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in a_topic_text[str(i)]:\n",
    "            tf_arr[i, j] = a_topic_text[str(i)][words[j]] / sum_theme[i]\n",
    "idf_arr = np.zeros(len(words))\n",
    "for j in range(len(words)):\n",
    "    idf_arr[j] = np.log(100 / sum([words[j] in a_topic_text[str(i)] for i in range(100)]))\n",
    "tfidf_text = np.array(tf_arr, copy = True)\n",
    "for k in range(len(words)):\n",
    "    tfidf_text[:,k] = tf_arr[:,k]  * idf_arr[k]\n",
    "words_dict_text = dict(zip(words, list(range(len(words)))))\n",
    "tfidf_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e67f5f2d7244ba85555980608d1696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21.0, 66.0, 172.0, 240.0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "stop_words3 = set(map(lambda x: x[0], stop_words1_90))\n",
    "stop_words4 = set(map(lambda x: x[0], bad_rare_words1))\n",
    "a = []\n",
    "a_len = []\n",
    "for i in tqdm_notebook(range(len(data_train[2]))):\n",
    "    text = data_train[2][i]\n",
    "    text = re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
    "    tmp = []\n",
    "    j = 0\n",
    "    for word in text:\n",
    "        if word not in stop_words3:\n",
    "            if word not in stop_words4:\n",
    "                try:\n",
    "                    if tfidf_text[:,words_dict_text[word]].sum()>0:\n",
    "                        if j < 240:\n",
    "                            tmp.append(tfidf_text[:,words_dict_text[word]])# / LA.norm(tfidf_text[:,words_dict_text[word]]))\n",
    "                            j+=1\n",
    "                except:\n",
    "                    pass\n",
    "    a_len.append(j)\n",
    "    while j < 240:\n",
    "        tmp.append(np.zeros(100))\n",
    "        j+= 1\n",
    "    a.append(np.array(tmp))\n",
    "pd.Series(a_len).quantile(0.1), pd.Series(a_len).quantile(0.5), pd.Series(a_len).quantile(0.9), pd.Series(a_len).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(a)\n",
    "b = []\n",
    "for i in range(data_train.shape[0]):\n",
    "    tmp = np.zeros(100)\n",
    "    for j in data_train[3][i].split(','):\n",
    "        tmp[int(j)] = 1\n",
    "    b.append(tmp)\n",
    "y_train = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6130dafdc0d64091a9c6f8926222cd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12593), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "stop_words3 = set(map(lambda x: x[0], stop_words1_90))\n",
    "stop_words4 = set(map(lambda x: x[0], bad_rare_words1))\n",
    "a = []\n",
    "a_len = []\n",
    "for i in tqdm_notebook(range(len(data_test[2]))):\n",
    "    text = data_test[2][i]\n",
    "    text = re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
    "    tmp = []\n",
    "    j = 0\n",
    "    for word in text:\n",
    "        if word not in stop_words3:\n",
    "            if word not in stop_words4:\n",
    "                try:\n",
    "                    if tfidf_text[:,words_dict_text[word]].sum()>0:\n",
    "                        if j < 240:\n",
    "                            tmp.append(tfidf_text[:,words_dict_text[word]])# / LA.norm(tfidf_text[:,words_dict_text[word]]))\n",
    "                            j+=1\n",
    "                except:\n",
    "                    pass\n",
    "    a_len.append(j)\n",
    "    while j < 240:\n",
    "        tmp.append(np.zeros(100))\n",
    "        j+= 1\n",
    "    a.append(np.array(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(a)\n",
    "b = []\n",
    "for i in range(data_test.shape[0]):\n",
    "    tmp = np.zeros(100)\n",
    "    for j in data_test[3][i].split(','):\n",
    "        tmp[int(j)] = 1\n",
    "    b.append(tmp)\n",
    "y_test = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "import keras\n",
    "hidden_dims1 = 200\n",
    "hidden_dims2 = 100\n",
    "reg = keras.regularizers.l1_l2(l1=0.0001, l2=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement=True\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential()\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv1D(100,\n",
    "#                      3,\n",
    "#                      padding='valid',\n",
    "#                      activation='selu',\n",
    "#                      strides=1,\n",
    "#                      kernel_regularizer=reg))\n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # model.add(Flatten())\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(200, activation='selu', input_shape=(100,), kernel_regularizer=reg))\n",
    "    model.add(Dropout(0.1))\n",
    "#     model.add(Dense(200, activation='selu'))\n",
    "#     model.add(Dropout(0.05))\n",
    "#     model.add(Dense(150, activation='selu', kernel_regularizer=reg))\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Nadam',\n",
    "                  metrics=['categorical_accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "\n",
    "    # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12593, 240, 100)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12593, 100)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12593, 100)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113335 samples, validate on 12593 samples\n",
      "Epoch 1/20\n",
      "113335/113335 [==============================] - 5s 48us/step - loss: 8.4504 - categorical_accuracy: 0.0709 - val_loss: 8.4801 - val_categorical_accuracy: 0.0741\n",
      "Epoch 2/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.4307 - categorical_accuracy: 0.0727 - val_loss: 8.4783 - val_categorical_accuracy: 0.0863\n",
      "Epoch 3/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.4294 - categorical_accuracy: 0.0750 - val_loss: 8.4790 - val_categorical_accuracy: 0.0741\n",
      "Epoch 4/20\n",
      "113335/113335 [==============================] - 5s 44us/step - loss: 8.4207 - categorical_accuracy: 0.0847 - val_loss: 8.4604 - val_categorical_accuracy: 0.0741\n",
      "Epoch 5/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.3812 - categorical_accuracy: 0.1042 - val_loss: 8.3974 - val_categorical_accuracy: 0.1248\n",
      "Epoch 6/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.3256 - categorical_accuracy: 0.1191 - val_loss: 8.3429 - val_categorical_accuracy: 0.1240\n",
      "Epoch 7/20\n",
      "113335/113335 [==============================] - 5s 46us/step - loss: 8.2774 - categorical_accuracy: 0.1242 - val_loss: 8.2951 - val_categorical_accuracy: 0.1264\n",
      "Epoch 8/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.2333 - categorical_accuracy: 0.1275 - val_loss: 8.2587 - val_categorical_accuracy: 0.1333\n",
      "Epoch 9/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.1917 - categorical_accuracy: 0.1342 - val_loss: 8.2097 - val_categorical_accuracy: 0.1445\n",
      "Epoch 10/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.1495 - categorical_accuracy: 0.1524 - val_loss: 8.1684 - val_categorical_accuracy: 0.1532\n",
      "Epoch 11/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.1089 - categorical_accuracy: 0.1652 - val_loss: 8.1271 - val_categorical_accuracy: 0.1721\n",
      "Epoch 12/20\n",
      "113335/113335 [==============================] - 5s 46us/step - loss: 8.0688 - categorical_accuracy: 0.1753 - val_loss: 8.0874 - val_categorical_accuracy: 0.1899\n",
      "Epoch 13/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.0354 - categorical_accuracy: 0.1801 - val_loss: 8.0566 - val_categorical_accuracy: 0.1849\n",
      "Epoch 14/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 8.0051 - categorical_accuracy: 0.1849 - val_loss: 8.0256 - val_categorical_accuracy: 0.2009\n",
      "Epoch 15/20\n",
      "113335/113335 [==============================] - 5s 44us/step - loss: 7.9775 - categorical_accuracy: 0.1882 - val_loss: 8.0190 - val_categorical_accuracy: 0.1745\n",
      "Epoch 16/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 7.9501 - categorical_accuracy: 0.1919 - val_loss: 7.9717 - val_categorical_accuracy: 0.1920\n",
      "Epoch 17/20\n",
      "113335/113335 [==============================] - 5s 46us/step - loss: 7.9273 - categorical_accuracy: 0.1945 - val_loss: 7.9472 - val_categorical_accuracy: 0.1990\n",
      "Epoch 18/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 7.9033 - categorical_accuracy: 0.1971 - val_loss: 7.9287 - val_categorical_accuracy: 0.2030\n",
      "Epoch 19/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 7.8789 - categorical_accuracy: 0.1992 - val_loss: 7.9056 - val_categorical_accuracy: 0.1954\n",
      "Epoch 20/20\n",
      "113335/113335 [==============================] - 5s 45us/step - loss: 7.8585 - categorical_accuracy: 0.2016 - val_loss: 7.8831 - val_categorical_accuracy: 0.2070\n",
      "Test loss: 7.883081209192887\n",
      "Test accuracy: 0.20701977289029228\n",
      "Train on 113335 samples, validate on 12593 samples\n",
      "Epoch 1/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8383 - categorical_accuracy: 0.2044 - val_loss: 7.8741 - val_categorical_accuracy: 0.2104\n",
      "Epoch 2/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8326 - categorical_accuracy: 0.2063 - val_loss: 7.8653 - val_categorical_accuracy: 0.2011\n",
      "Epoch 3/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8254 - categorical_accuracy: 0.2051 - val_loss: 7.8638 - val_categorical_accuracy: 0.2108\n",
      "Epoch 4/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8190 - categorical_accuracy: 0.2056 - val_loss: 7.8549 - val_categorical_accuracy: 0.2096\n",
      "Epoch 5/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8138 - categorical_accuracy: 0.2071 - val_loss: 7.8545 - val_categorical_accuracy: 0.2021\n",
      "Epoch 6/10\n",
      "113335/113335 [==============================] - 1s 13us/step - loss: 7.8066 - categorical_accuracy: 0.2061 - val_loss: 7.8388 - val_categorical_accuracy: 0.2068\n",
      "Epoch 7/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.8009 - categorical_accuracy: 0.2076 - val_loss: 7.8341 - val_categorical_accuracy: 0.2090\n",
      "Epoch 8/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.7932 - categorical_accuracy: 0.2078 - val_loss: 7.8247 - val_categorical_accuracy: 0.2062\n",
      "Epoch 9/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.7867 - categorical_accuracy: 0.2080 - val_loss: 7.8217 - val_categorical_accuracy: 0.2057\n",
      "Epoch 10/10\n",
      "113335/113335 [==============================] - 1s 12us/step - loss: 7.7792 - categorical_accuracy: 0.2081 - val_loss: 7.8136 - val_categorical_accuracy: 0.2204\n",
      "Test loss: 7.813591341024583\n",
      "Test accuracy: 0.22036051774825102\n",
      "Train on 113335 samples, validate on 12593 samples\n",
      "Epoch 1/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7715 - categorical_accuracy: 0.2102 - val_loss: 7.8060 - val_categorical_accuracy: 0.2111\n",
      "Epoch 2/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7693 - categorical_accuracy: 0.2093 - val_loss: 7.8032 - val_categorical_accuracy: 0.2150\n",
      "Epoch 3/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7670 - categorical_accuracy: 0.2112 - val_loss: 7.8033 - val_categorical_accuracy: 0.2137\n",
      "Epoch 4/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7656 - categorical_accuracy: 0.2096 - val_loss: 7.7994 - val_categorical_accuracy: 0.2155\n",
      "Epoch 5/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7635 - categorical_accuracy: 0.2107 - val_loss: 7.7985 - val_categorical_accuracy: 0.2126\n",
      "Epoch 6/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7626 - categorical_accuracy: 0.2091 - val_loss: 7.7970 - val_categorical_accuracy: 0.2156\n",
      "Epoch 7/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7595 - categorical_accuracy: 0.2102 - val_loss: 7.7943 - val_categorical_accuracy: 0.2144\n",
      "Epoch 8/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7567 - categorical_accuracy: 0.2110 - val_loss: 7.7935 - val_categorical_accuracy: 0.2092\n",
      "Epoch 9/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7546 - categorical_accuracy: 0.2106 - val_loss: 7.7914 - val_categorical_accuracy: 0.2116\n",
      "Epoch 10/10\n",
      "113335/113335 [==============================] - 0s 4us/step - loss: 7.7526 - categorical_accuracy: 0.2114 - val_loss: 7.7887 - val_categorical_accuracy: 0.2113\n",
      "Test loss: 7.788711337775127\n",
      "Test accuracy: 0.2113078694518741\n",
      "Train on 113335 samples, validate on 12593 samples\n",
      "Epoch 1/5\n",
      "113335/113335 [==============================] - 0s 2us/step - loss: 7.7525 - categorical_accuracy: 0.2116 - val_loss: 7.7867 - val_categorical_accuracy: 0.2142\n",
      "Epoch 2/5\n",
      "113335/113335 [==============================] - 0s 2us/step - loss: 7.7506 - categorical_accuracy: 0.2110 - val_loss: 7.7858 - val_categorical_accuracy: 0.2120\n",
      "Epoch 3/5\n",
      "113335/113335 [==============================] - 0s 2us/step - loss: 7.7506 - categorical_accuracy: 0.2109 - val_loss: 7.7856 - val_categorical_accuracy: 0.2118\n",
      "Epoch 4/5\n",
      "113335/113335 [==============================] - 0s 2us/step - loss: 7.7495 - categorical_accuracy: 0.2106 - val_loss: 7.7848 - val_categorical_accuracy: 0.2135\n",
      "Epoch 5/5\n",
      "113335/113335 [==============================] - 0s 2us/step - loss: 7.7489 - categorical_accuracy: 0.2103 - val_loss: 7.7851 - val_categorical_accuracy: 0.2136\n",
      "Test loss: 7.785141869154743\n",
      "Test accuracy: 0.21361073612383472\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.sum(axis=1), y_train,\n",
    "                  batch_size=128,\n",
    "                  epochs=20,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test.sum(axis=1), y_test))\n",
    "score = model.evaluate(X_test.sum(axis=1), y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# print('Test top_k_accuracy:', score[2])\n",
    "history = model.fit(X_train.sum(axis=1), y_train,\n",
    "                  batch_size=512,\n",
    "                  epochs=10,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test.sum(axis=1), y_test))\n",
    "score = model.evaluate(X_test.sum(axis=1), y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# print('Test top_k_accuracy:', score[2])\n",
    "history = model.fit(X_train.sum(axis=1), y_train,\n",
    "                  batch_size=2048,\n",
    "                  epochs=10,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test.sum(axis=1), y_test))\n",
    "score = model.evaluate(X_test.sum(axis=1), y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# print('Test top_k_accuracy:', score[2])\n",
    "history = model.fit(X_train.sum(axis=1), y_train,\n",
    "                  batch_size=8196,\n",
    "                  epochs=5,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_test.sum(axis=1), y_test))\n",
    "score = model.evaluate(X_test.sum(axis=1), y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# print('Test top_k_accuracy:', score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x7f8de8de2b00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/justm/anaconda3/envs/uw_env/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n",
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x7f8de8de2b00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/justm/anaconda3/envs/uw_env/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297258\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import gc\n",
    "import tensorflow\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
